{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n",
      "number of negative instances: 94168 \n",
      "number of positive instances: 2117 \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import optunity\n",
    "import optunity.cross_validation\n",
    "import optunity.metrics\n",
    "import numpy as np\n",
    "import sklearn.svm\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals import six\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import _name_estimators\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "class MajorityVoteClassifier(BaseEstimator,\n",
    "                             ClassifierMixin):\n",
    "    \"\"\" A majority vote ensemble classifier\n",
    "    Parameters\n",
    "    ----------\n",
    "    classifiers : array-like, shape = [n_classifiers]\n",
    "      Different classifiers for the ensemble\n",
    "    vote : str, {'classlabel', 'probability'} (default='label')\n",
    "      If 'classlabel' the prediction is based on the argmax of\n",
    "        class labels. Else if 'probability', the argmax of\n",
    "        the sum of probabilities is used to predict the class label\n",
    "        (recommended for calibrated classifiers).\n",
    "    weights : array-like, shape = [n_classifiers], optional (default=None)\n",
    "      If a list of `int` or `float` values are provided, the classifiers\n",
    "      are weighted by importance; Uses uniform weights if `weights=None`.\n",
    "    \"\"\"\n",
    "    def __init__(self, classifiers, vote='classlabel', weights=None):\n",
    "\n",
    "        self.classifiers = classifiers\n",
    "        self.named_classifiers = {key: value for key, value\n",
    "                                  in _name_estimators(classifiers)}\n",
    "        self.vote = vote\n",
    "        self.weights = weights\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit classifiers.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Matrix of training samples.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Vector of target class labels.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        if self.vote not in ('probability', 'classlabel'):\n",
    "            raise ValueError(\"vote must be 'probability' or 'classlabel'\"\n",
    "                             \"; got (vote=%r)\"\n",
    "                             % self.vote)\n",
    "\n",
    "        if self.weights and len(self.weights) != len(self.classifiers):\n",
    "            raise ValueError('Number of classifiers and weights must be equal'\n",
    "                             '; got %d weights, %d classifiers'\n",
    "                             % (len(self.weights), len(self.classifiers)))\n",
    "\n",
    "        # Use LabelEncoder to ensure class labels start with 0, which\n",
    "        # is important for np.argmax call in self.predict\n",
    "        self.lablenc_ = LabelEncoder()\n",
    "        self.lablenc_.fit(y)\n",
    "        self.classes_ = self.lablenc_.classes_\n",
    "        self.classifiers_ = []\n",
    "        for clf in self.classifiers:\n",
    "            fitted_clf = clone(clf).fit(X, self.lablenc_.transform(y))\n",
    "            self.classifiers_.append(fitted_clf)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict class labels for X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Matrix of training samples.\n",
    "        Returns\n",
    "        ----------\n",
    "        maj_vote : array-like, shape = [n_samples]\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        if self.vote == 'probability':\n",
    "            maj_vote = np.argmax(self.predict_proba(X), axis=1)\n",
    "        else:  # 'classlabel' vote\n",
    "\n",
    "            #  Collect results from clf.predict calls\n",
    "            predictions = np.asarray([clf.predict(X)\n",
    "                                      for clf in self.classifiers_]).T\n",
    "\n",
    "            maj_vote = np.apply_along_axis(\n",
    "                lambda x:\n",
    "                np.argmax(np.bincount(x,\n",
    "                                      weights=self.weights)),\n",
    "                axis=1,\n",
    "                arr=predictions)\n",
    "        maj_vote = self.lablenc_.inverse_transform(maj_vote)\n",
    "        return maj_vote\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\" Predict class probabilities for X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        Returns\n",
    "        ----------\n",
    "        avg_proba : array-like, shape = [n_samples, n_classes]\n",
    "            Weighted average probability for each class per sample.\n",
    "        \"\"\"\n",
    "        probas = np.asarray([clf.predict_proba(X)\n",
    "                             for clf in self.classifiers_])\n",
    "        avg_proba = np.average(probas, axis=0, weights=self.weights)\n",
    "        return avg_proba\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\" Get classifier parameter names for GridSearch\"\"\"\n",
    "        if not deep:\n",
    "            return super(MajorityVoteClassifier, self).get_params(deep=False)\n",
    "        else:\n",
    "            out = self.named_classifiers.copy()\n",
    "            for name, step in six.iteritems(self.named_classifiers):\n",
    "                for key, value in six.iteritems(step.get_params(deep=True)):\n",
    "                    out['%s__%s' % (name, key)] = value\n",
    "        return out\n",
    "    \n",
    "def creatematrix(features, kmer):\n",
    "    stat = pd.read_csv(features, sep=\",\")\n",
    "    kmer = pd.read_csv(kmer, sep=\"\\t\", header = None)\n",
    "    kmer = kmer.iloc[:, :-1]\n",
    "    id2 = stat.id.str.split(\"-\",expand=True) # split the string to get label\n",
    "    id2 = id2.iloc[:, :-1]\n",
    "    stat2 = stat.iloc[:, 1:]\n",
    "    df = pd.concat([stat2.reset_index(drop=True), kmer], axis=1) # concat kmerand stat matrix\n",
    "    df = pd.concat([id2, df], axis=1)\n",
    "    df.columns.values[0] = \"label\"\n",
    "    # encoding class labels as integers\n",
    "    df.loc[df.label == 'positive', 'label'] = 1\n",
    "    df.loc[df.label == 'negative', 'label'] = 0\n",
    "    return df\n",
    "\n",
    "print(\"load data\")\n",
    "dat = creatematrix('../dat/train.features.clear2.csv', '../dat/train.features.kmer')\n",
    "\n",
    "# check if the dataset is inbalanced\n",
    "dat.pos = dat.loc[dat['label'] == 1]\n",
    "dat.neg = dat.loc[dat['label'] == 0]\n",
    "\n",
    "#dat.pos.shape\n",
    "print('number of negative instances: %d ' % (dat.neg.shape[0]))\n",
    "print('number of positive instances: %d ' % (dat.pos.shape[0]))\n",
    "#num = min(dat.neg.shape[0],dat.pos.shape[0])\n",
    "#print('limit dataset size to %d' % (num))\n",
    "\n",
    "# generate a random subset of both with the size of $num\n",
    "\n",
    "#posrand = dat.pos.sample(n=num)\n",
    "#negrand = dat.neg.sample(n=num)\n",
    "\n",
    "#df = posrand.copy()\n",
    "#dat = df.append(negrand)\n",
    "\n",
    "# split to test and training set\n",
    "y = dat['label'].tolist() # extract label\n",
    "X = dat.drop(dat.columns[[0]], 1) # remove label\n",
    "\n",
    "#data = X.as_matrix()\n",
    "#labels = [True] * len(posrand) + [False] * len(negrand)\n",
    "print('done')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balanced_subsample(x,y,subsample_size=1.0):\n",
    "\n",
    "    class_xs = []\n",
    "    min_elems = None\n",
    "\n",
    "    for yi in np.unique(y):\n",
    "        elems = x[(y == yi)]\n",
    "        class_xs.append((yi, elems))\n",
    "        if min_elems == None or elems.shape[0] < min_elems:\n",
    "            min_elems = elems.shape[0]\n",
    "\n",
    "    use_elems = min_elems\n",
    "    if subsample_size < 1:\n",
    "        use_elems = int(min_elems*subsample_size)\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "    for ci,this_xs in class_xs:\n",
    "        if len(this_xs) > use_elems:\n",
    "            this_xs = this_xs.reindex(np.random.permutation(this_xs.index))\n",
    "            \n",
    "        x_ = this_xs[:use_elems]\n",
    "        y_ = np.empty(use_elems)\n",
    "        y_.fill(ci)\n",
    "\n",
    "        xs.append(x_)\n",
    "        ys.append(y_)\n",
    "\n",
    "    xs = pd.concat(xs)\n",
    "    ys = pd.Series(data=np.concatenate(ys),name='target')\n",
    "\n",
    "    return xs,ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_sub, y_sub = balanced_subsample(X, y, subsample_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 48.56 seconds for 10 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.711 (std: 0.033)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 8, 'n_estimators': 500, 'min_samples_split': 36, 'criterion': 'gini', 'max_features': 19, 'max_depth': 3}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.694 (std: 0.009)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 13, 'n_estimators': 500, 'min_samples_split': 43, 'criterion': 'gini', 'max_features': 3, 'max_depth': None}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.692 (std: 0.031)\n",
      "Parameters: {'bootstrap': False, 'min_samples_leaf': 38, 'n_estimators': 500, 'min_samples_split': 46, 'criterion': 'entropy', 'max_features': 17, 'max_depth': 4}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"max_depth\": [5, 4, 3, None],\n",
    "              \"n_estimators\": [500, 2000],\n",
    "              \"max_features\": sp_randint(1, 50),\n",
    "              \"min_samples_split\": sp_randint(2, 50),\n",
    "              \"min_samples_leaf\": sp_randint(1, 50),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 10\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist, scoring = 'accuracy',\n",
    "                                   n_iter=n_iter_search, n_jobs=-1, refit=True)\n",
    "\n",
    "start = time()\n",
    "random_search.fit(X_sub, y_sub)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.cv_results_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.00000000e-09,   1.00000000e-08,   1.00000000e-07,\n",
       "         1.00000000e-06,   1.00000000e-05,   1.00000000e-04,\n",
       "         1.00000000e-03,   1.00000000e-02,   1.00000000e-01,\n",
       "         1.00000000e+00,   1.00000000e+01,   1.00000000e+02,\n",
       "         1.00000000e+03])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(-9, 3, 13)\n",
    "np.logspace(-2, 10, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/scipy/optimize/linesearch.py:285: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/optimize/linesearch.py:285: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/optimize/linesearch.py:285: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/optimize/linesearch.py:285: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 42.41 seconds for 200 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.704 (std: 0.041)\n",
      "Parameters: {'C': 0.01, 'dual': False, 'tol': 1.0000000000000001e-05, 'solver': 'sag'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.701 (std: 0.044)\n",
      "Parameters: {'C': 0.001, 'dual': False, 'tol': 1.0, 'solver': 'liblinear'}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.699 (std: 0.043)\n",
      "Parameters: {'C': 0.01, 'dual': False, 'tol': 9.9999999999999995e-07, 'solver': 'sag'}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.699 (std: 0.046)\n",
      "Parameters: {'C': 0.01, 'dual': False, 'tol': 1e-08, 'solver': 'sag'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = LogisticRegression(penalty='l2')\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"C\": np.logspace(-9, 3, 13),\n",
    "              \"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag'],\n",
    "              \"dual\":[False],\n",
    "              \"tol\": np.logspace(-9, 3, 13)\n",
    "            }\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 200\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist, scoring = 'accuracy',\n",
    "                                   n_iter=n_iter_search, n_jobs=-1, refit=True)\n",
    "\n",
    "start = time()\n",
    "random_search.fit(X_sub, y_sub)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.65 (+/- 0.06)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "\n",
    "scores = cross_val_score(estimator=clf,\n",
    "                             X=X_sub,\n",
    "                             y=y_sub,\n",
    "                             cv=10,\n",
    "                             scoring='accuracy')\n",
    "print(\"ROC AUC: %0.2f (+/- %0.2f)\"\n",
    "          % (scores.mean(), scores.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 18.17 seconds for 30 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.692 (std: 0.030)\n",
      "Parameters: {'kernel': 'linear', 'C': 0.0014801919594828084, 'gamma': 362.03867196749627}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.671 (std: 0.025)\n",
      "Parameters: {'kernel': 'rbf', 'C': 6.0628662660414046, 'gamma': 0.0029603839189656098}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.671 (std: 0.035)\n",
      "Parameters: {'kernel': 'linear', 'C': 0.0029603839189656098, 'gamma': 891.44377681518768}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC()\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'C': pow(2.0, np.arange(-10, 11, 0.1)), 'gamma': pow(2.0, np.arange(-10, 11, 0.1)),\n",
    "'kernel': ['linear', 'rbf']}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 30\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist, scoring = 'accuracy',\n",
    "                                   n_iter=n_iter_search, n_jobs=-1, refit=True)\n",
    "\n",
    "start = time()\n",
    "random_search.fit(X_sub, y_sub)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.cv_results_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
